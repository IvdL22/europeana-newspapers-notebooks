{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606ef29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.clarin-pl.eu\n",
      "Requirement already satisfied: lpmn_client in /opt/conda/lib/python3.9/site-packages (1.4.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from lpmn_client) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from lpmn_client) (4.64.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from lpmn_client) (6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->lpmn_client) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->lpmn_client) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->lpmn_client) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->lpmn_client) (2.0.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://pypi.clarin-pl.eu lpmn_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae044b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in /opt/conda/lib/python3.9/site-packages (5.3.0)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.9/site-packages (4.8.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.9/site-packages (from nbformat) (4.4.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.9/site-packages (from nbformat) (2.15.3)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.9/site-packages (from nbformat) (4.9.2)\n",
      "Requirement already satisfied: traitlets>=4.1 in /opt/conda/lib/python3.9/site-packages (from nbformat) (5.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat) (21.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nbformat lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f28157ae-a393-4406-8381-c9c1921ca2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Utilities\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "from io import BytesIO\n",
    "import json\n",
    "import logging\n",
    "from lxml import etree\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\"\"\"\n",
    "    Globals\n",
    "\"\"\"\n",
    "\n",
    " # Poland\n",
    "set_id = '9200357'\n",
    "\n",
    "data_dir = f'{os.path.expanduser(\"~\")}/work/data'\n",
    "metadata_dir = f'{os.path.expanduser(\"~\")}/work/temp/metadata/{set_id}'\n",
    "nsmap = {\"cmd\": \"http://www.clarin.eu/cmd/1\",\n",
    "         \"cmdp_text\": \"http://www.clarin.eu/cmd/1/profiles/clarin.eu:cr1:p_1633000337997\"}\n",
    "output_file = f'{os.path.expanduser(\"~\")}/work/output'\n",
    "\n",
    "\n",
    "with open(f'{data_dir}/{set_id}/id_file_map.json', 'r') as id_filename_map_file:\n",
    "    id_filename_map = json.load(id_filename_map_file)\n",
    "\n",
    "YYYY_MM_DD = re.compile(r\"(?P<year>[0-9]{4})-(?P<month>[0-9]{1,2})-(?P<day>[0-9]{1,2})\")\n",
    "MAX_NER_TASK_SIZE = 2000000\n",
    "\n",
    "\"\"\"\n",
    "    Metadata printing\n",
    "\"\"\"\n",
    "def print_xml(tree, declaration: bool = False):\n",
    "    print(etree.tostring(tree, encoding='UTF-8', xml_declaration=declaration, pretty_print=True).decode())\n",
    "\n",
    "\"\"\"\n",
    "    Data acess\n",
    "\"\"\"\n",
    "def get_resource_file(identifier):\n",
    "    \"\"\"\n",
    "        Resolves Europeana subresource identifier to it's local location. \n",
    "        \n",
    "        :param str identifier: Europeana subresource identifier\n",
    "        :return str: Path to local location of the resource\n",
    "    \"\"\"\n",
    "    if identifier in id_filename_map:\n",
    "        filename = id_filename_map[identifier]\n",
    "        return f'{data_dir}/{set_id}/{filename}'\n",
    "\n",
    "def get_date_from_metadata(metadata_tree):\n",
    "    dates = metadata_tree.xpath(\"///cmdp_text:TemporalCoverage/cmdp_text:Start/cmdp_text:date/text()\", namespaces=nsmap)\n",
    "    dates = [YYYY_MM_DD.match(date) for date in dates]\n",
    "    dates = [datetime.date(int(date.group(\"year\")), int(date.group(\"month\")), int(date.group(\"day\"))) for date in dates]\n",
    "    return dates\n",
    "    \n",
    "def get_description_from_metadata(metadata_tree):\n",
    "    descriptions = metadata_tree.xpath('//cmdp_text:TextResource/cmdp_text:Description/cmdp_text:description/text()', namespaces=nsmap)\n",
    "    if len(descriptions) > 0:\n",
    "        return descriptions[0]\n",
    "    \n",
    "def get_resource_ids_from_metadata(metadata_tree):\n",
    "    ids = metadata_tree.xpath('//cmdp_text:SubresourceDescription/cmdp_text:IdentificationInfo/cmdp_text:identifier/text()', namespaces=nsmap)\n",
    "    # The result can be any number of identifiers. We do want to filter the values a bit: only the numeric identifiers are useful \n",
    "    # to us so we use the special syntax below to make a new list by picking only the matching values from the query results list\n",
    "    return [id for id in ids if id.isnumeric()]\n",
    "\n",
    "def get_title_from_metadata(metadata_tree):\n",
    "    # Get all the values from the xpath\n",
    "    titles = metadata_tree.xpath('//cmdp_text:TextResource/cmdp_text:TitleInfo/cmdp_text:title/text()', namespaces=nsmap)\n",
    "    # Check if there is an actual value\n",
    "    if len(titles) > 0:\n",
    "        # Return the first (assuming only) value\n",
    "        return titles[0]\n",
    "\n",
    "def unpack_metadata(set_id, target_dir):\n",
    "    # Construct the address of the .zip file with the metadata for one set\n",
    "    md_zip_url = f'https://europeana-oai.clarin.eu/metadata/fulltext-aggregation/{set_id}.zip'\n",
    "    \n",
    "    # Retrieve the .zip file\n",
    "    print(f'Retrieving {md_zip_url}')\n",
    "    resp = requests.get(md_zip_url)\n",
    "    zipfile = ZipFile(BytesIO(resp.content))\n",
    "    \n",
    "    # Uncompress the .zip into the target directory\n",
    "    print(f'Extracting content in {target_dir}')\n",
    "    zipfile.extractall(path=target_dir)\n",
    "    print('Done')\n",
    "\n",
    "\"\"\"\n",
    "    Zip/Unzip\n",
    "\"\"\"\n",
    "def zip_file(input_path, output_path=\"\"):\n",
    "    \"\"\"\n",
    "        Zips input file/directory\n",
    "        \n",
    "        :param str input_path: path to file to be zipped, if file is directory, entire dir gets zipped\n",
    "        :param str output_path: path to location, where to save the archive. If empty, zip archive uses same location and name as input\n",
    "        :returns str: path to archive\n",
    "    \"\"\"\n",
    "    if not output_path:\n",
    "        output_path = f\"{os.path.dirname(input_path)}/{os.path.basename(input_path).split('.')[0]}.zip\"\n",
    "    with ZipFile(output_path, 'r') as zip_handle:\n",
    "        logger.info(f'Zipping {input_path} to {output_path}')\n",
    "        if os.path.isdir(input_path):\n",
    "            _zip_dir(input_path, zip_handle)\n",
    "        else:\n",
    "            zip_handle.write(input_path, os.path.basename(input_path))\n",
    "            # _zip_chunker(input_path)\n",
    "        \n",
    "    return output_path\n",
    "        \n",
    "def unzip_file(input_path, output_path=\"\"):\n",
    "    \"\"\"\n",
    "        Unzips input .zip file\n",
    "        \n",
    "        :param str input_path: path to file to be unzipped\n",
    "        :param str output_path: path to location, where to unpack the archive. If empty, archive is extracted at its location.\n",
    "    \"\"\"\n",
    "    if not output_path:\n",
    "        output_path = f\"{os.path.dirname(input_path)}/{os.path.basename(input_path).split('.')[0]}\"\n",
    "        print(\"$$\")\n",
    "        print(output_path)\n",
    "        print(input_path)\n",
    "    with ZipFile(input_path, 'r') as zip_ref:\n",
    "        logger.info(f'Unzipping {input_path} to {output_path}')\n",
    "        zip_ref.extractall()\n",
    "        extracted_files_paths = zip_ref.namelist()\n",
    "        \n",
    "    return [os.path.join(output_path, extracted_file_path) for extracted_file_path in extracted_files_paths]\n",
    "\n",
    "def _zip_dir(input_dir_path, zip_handle):\n",
    "    for dirname, subdirs, files in os.walk(input_dir_path):\n",
    "        for filename in files:          \n",
    "            logger.info(f'Zipping {dirname}/{filename}')\n",
    "            # _zip_chunker(os.path.join(dirname, filename))\n",
    "            zip_handle.write(os.path.join(dirname, filename), \n",
    "                             os.path.relpath(os.path.join(dirname, filename), \n",
    "                                             os.path.join(input_dir_path, '..')))\n",
    "            \n",
    "\"\"\"\n",
    "    Safety\n",
    "\"\"\"\n",
    "def _check_task_size(resources):\n",
    "    size = sum([os.path.getsize(resource) for resource in resources])\n",
    "    if size > MAX_NER_TASK_SIZE:\n",
    "        raise TaskTooBigError\n",
    "        \n",
    "class TaskTooBigError(Exception):\n",
    "    \"\"\"\n",
    "        Exception raised for tasks with too big payload.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, max_size):\n",
    "        self.message = f\"Tasks payload is too big, it has {size} and maximum is {max_size}\"\n",
    "        super().__init__(self.message)\n",
    "   \n",
    "\"\"\"\n",
    "    Logging\n",
    "\"\"\"\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76fdd3-6489-4f92-b590-b2e322e88683",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with Liner2\n",
    "\n",
    "In this section we will present how to use Europeana bibliographic resources with Named Entity Recognition (NER) tool for Polish using [lpmn_client](https://wiki.clarin-pl.eu/en/nlpws/lpmn_client) and [Liner2](https://github.com/CLARIN-PL/Liner2). Due to the server side limitation, we ensure 2M limit on size on the task with function defined below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4eb08a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function for tasking lpmn client with Liner2 NER pipeline with task size control \n",
    "\"\"\"\n",
    "\n",
    "from lpmn_client import download_file, upload_file\n",
    "from lpmn_client import Task\n",
    "\n",
    "\n",
    "def liner2_NER(resources, names=[]):\n",
    "    \"\"\"\n",
    "        Wrap over CLARIN-PL lpmn client with control of the task size in order to avoid jamming the task queue on the server side\n",
    "        \n",
    "        :param list resources: list of paths to the resources to be processed\n",
    "        :returns list: list of paths to the output zip files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Size check\n",
    "    _check_task_size(resources)\n",
    "    # Upload reasources to task queue\n",
    "    job_ids = [upload_file(resource_file) for resource_file in resources]\n",
    "    # Specify pipeline \n",
    "    t = Task(\"any2txt|wcrft2|liner2\")\n",
    "    # Run uploaded tasks with pipeline\n",
    "    output_file_ids = [t.run(job_id, verbose=True) for job_id in job_ids]\n",
    "    if names:\n",
    "        liner2_output = [download_file(output_file_id, output_file, filename) for output_file_id, filename in zip(output_file_ids, names)]\n",
    "    else:\n",
    "        liner2_output = [download_file(output_file_id, output_file) for output_file_id in output_file_ids]\n",
    "    return liner2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "caaf0eb0-9333-480f-9f08-985e16ced020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving https://europeana-oai.clarin.eu/metadata/fulltext-aggregation/9200357.zip\n",
      "Extracting content in /home/jovyan/work/temp/metadata/9200357\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Let's select resources. We will use Izraelita newspapers from 1910-1913 to investigate most frequent Named Entities over the years on January. \n",
    "\"\"\"\n",
    "\n",
    "# Prepare metadata of the collection\n",
    "unpack_metadata(set_id, metadata_dir)\n",
    "izraelita_metadata_files = [f\"{metadata_dir}/Izraelita_1910.xml\",\n",
    "                            f\"{metadata_dir}/Izraelita_1911.xml\",\n",
    "                            f\"{metadata_dir}/Izraelita_1912.xml\",\n",
    "                            f\"{metadata_dir}/Izraelita_1913.xml\",\n",
    "                           ]\n",
    "\n",
    "# Use getters to obtain date and id from metadata\n",
    "izraelita_metadata_trees = [etree.parse(izraelita_metadata_file) for izraelita_metadata_file in izraelita_metadata_files]\n",
    "ids = [get_resource_ids_from_metadata(izraelita_metadata_tree) for izraelita_metadata_tree in izraelita_metadata_trees]\n",
    "dates = [get_date_from_metadata(izraelita_metadata_tree) for izraelita_metadata_tree in izraelita_metadata_trees]\n",
    "\n",
    "# Get all issues from January\n",
    "ids_dates_january = []\n",
    "for _ids, _dates in zip (ids, dates):\n",
    "    year = []\n",
    "    for _id, date in zip(_ids, _dates):\n",
    "        if date.month==1:\n",
    "            year.append((_id, date))\n",
    "    ids_dates_january.append(year)\n",
    "\n",
    "# Get dates of first issues per year\n",
    "first_issues_date_per_year = {min([date for _, date in year]) for year in ids_dates_january}\n",
    "\n",
    "# Filter ids of first issues per year \n",
    "id_date_first_issue_january = [(_id, date) for year in ids_dates_january for _ids, date in year if date in first_issues_date_per_year]\n",
    "\n",
    "# Map id to reasource\n",
    "resources_dates = [(get_resource_file(_id), date) for _id, date in id_date_first_issue_january]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c3407fbc-f163-454d-89ea-697a218a9cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100.0/100 [00:03<00:00, 27.79it/s]\n",
      "100%|██████████| 100.0/100 [00:03<00:00, 28.03it/s]\n",
      "100%|██████████| 100.0/100 [00:03<00:00, 27.91it/s]\n",
      "100%|██████████| 100.0/100 [00:02<00:00, 34.96it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Let's use selected resources\n",
    "\"\"\"\n",
    "\n",
    "for resource, date in resources_dates:\n",
    "    liner2_NER([resource], [date])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b3cad33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/output/1910-01-01\n",
      "/home/jovyan/work/output/1911-01-01\n",
      "/home/jovyan/work/output/1912-01-01\n",
      "/home/jovyan/work/output/1913-01-03\n",
      "$$\n",
      "/home/jovyan/work/output/1910-01-01\n",
      "/home/jovyan/work/output/1910-01-01\n",
      "$$\n",
      "/home/jovyan/work/output/1911-01-01\n",
      "/home/jovyan/work/output/1911-01-01\n",
      "$$\n",
      "/home/jovyan/work/output/1912-01-01\n",
      "/home/jovyan/work/output/1912-01-01\n",
      "$$\n",
      "/home/jovyan/work/output/1913-01-03\n",
      "/home/jovyan/work/output/1913-01-03\n",
      "[['/home/jovyan/work/output/1910-01-01/home%jovyan%work%data%9200357%BibliographicResource_3000095241512.txt'], ['/home/jovyan/work/output/1911-01-01/home%jovyan%work%data%9200357%BibliographicResource_3000095241512.txt'], ['/home/jovyan/work/output/1912-01-01/home%jovyan%work%data%9200357%BibliographicResource_3000095241512.txt'], ['/home/jovyan/work/output/1913-01-03/home%jovyan%work%data%9200357%BibliographicResource_3000095241512.txt']]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [123]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# We flatten list [[file_path]] to [file_path]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(liner2_output_file_paths)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mliner2_output_file_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m liner2_output_file:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(liner2_output_file)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "for _, date in resources_dates:\n",
    "    print(f\"{output_file}/{date}\")\n",
    "liner2_output_file_paths = [unzip_file(f\"{output_file}/{date}\") for _, date in resources_dates]\n",
    "# We flatten list [[file_path]] to [file_path]\n",
    "\n",
    "print(liner2_output_file_paths)\n",
    "\n",
    "with open(liner2_output_file_paths[0], 'r') as liner2_output_file:\n",
    "    print(liner2_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903478f-1c5d-470b-8897-c41ee8cf9145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0f9a9-0af3-492c-a0f7-6c731a9e6bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e21de5-e927-4493-87f0-dbdbe2d3c5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934f8cc-b063-40c1-bc9b-2aec68c28cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
