{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0338b479",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data selection and resource access\n",
    "\n",
    "In this notebook you will:\n",
    "- filter and collect resources based on properties in the metadata\n",
    "- carry out simple analysis tasks on custom data collections such as counting files or words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e938dea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.9/site-packages (4.8.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.19.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (4.29.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the libraries we will need in this notebook are installed\n",
    "!pip install lxml numpy matplotlib\n",
    "\n",
    "# Define some handy defaults\n",
    "nsmap = {\"cmd\": \"http://www.clarin.eu/cmd/1\",\n",
    "         \"cmdp_text\": \"http://www.clarin.eu/cmd/1/profiles/clarin.eu:cr1:p_1633000337997\"}\n",
    "\n",
    "from os.path import expanduser\n",
    "data_dir = expanduser(\"~\") + '/work/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d266b41-cd0c-42f3-b423-a6f2b5cca2aa",
   "metadata": {},
   "source": [
    "# Retrieving metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12487963-05e1-4e8c-8db5-b9c115485116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "'''\n",
    "    This function gets and unpacks a metadata archive\n",
    "'''\n",
    "def unpack_metadata(set_id, target_dir):\n",
    "    # construct the address of the .zip file with the metadata for one set\n",
    "    md_zip_url = f'https://europeana-oai.clarin.eu/metadata/fulltext-aggregation/{set_id}.zip'\n",
    "    \n",
    "    # retrieve the .zip file\n",
    "    print(f'Retrieving {md_zip_url}')\n",
    "    resp = requests.get(md_zip_url)\n",
    "    zipfile = ZipFile(BytesIO(resp.content))\n",
    "    \n",
    "    print(f'Extracting content in {target_dir}')\n",
    "    zipfile.extractall(path=target_dir)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4d40283-2211-4b92-beaa-a2f55764ded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving https://europeana-oai.clarin.eu/metadata/fulltext-aggregation/9200357.zip\n",
      "Extracting content in /home/jovyan/work/temp/metadata/9200357\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from os.path import expanduser\n",
    "\n",
    "'''\n",
    "    We now call the function for the set of Polish newspapers and extract them in a temporary directory so that we can use it.\n",
    "'''\n",
    "\n",
    "set_id = '9200357' # Poland\n",
    "metadata_dir = f'{expanduser(\"~\")}/work/temp/metadata/{set_id}'\n",
    "unpack_metadata(set_id, metadata_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc2fa0b-94ae-4935-8b0b-ec12ab439e26",
   "metadata": {},
   "source": [
    "# Data segmentation using metadata properties\n",
    "Now we have the metadata available, we want to use the properties in the metadata to create useful data segments. For The metadata tells us, for instance, the year of publication of each newspaper. \n",
    "\n",
    "Thus, we can narrow down our selection of text files and at the same time, using the resource identifiers that are also found in the metadata, create a 'map' that allows us to look up specific files based on a property we are interested in, in this case year of publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e183b84-e5c0-4d4e-a790-ba8725eaa90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "'''\n",
    "    This function returns the publication year and resource identifiers for a metadata record.\n",
    "    If the record is not a text resource record, it returns nothing\n",
    "'''\n",
    "def get_year_and_ids(record_file):\n",
    "    xml_tree = etree.parse(record_file)\n",
    "    text_resource_root_node = xml_tree.xpath('//cmd:Components/cmdp_text:TextResource', namespaces=nsmap)\n",
    "\n",
    "    if len(text_resource_root_node) < 1:\n",
    "        # Metadata in file is not for a text resource...\n",
    "        return None\n",
    "    else:\n",
    "        # Get the temporal coverage information\n",
    "        year_text_node = xml_tree.xpath(\n",
    "            '//cmdp_text:TextResource/cmdp_text:TemporalCoverage/cmdp_text:Start/cmdp_text:year/text()', namespaces=nsmap)\n",
    "        if len(year_text_node) < 1:\n",
    "            # No publication year found... this is unexpected!\n",
    "            print(f'Warning: year information not found in {record_file}')\n",
    "            return None\n",
    "        else:\n",
    "            year = year_text_node[0]\n",
    "            # get the record identifiers\n",
    "            record_ids_text_nodes = xml_tree.xpath(\n",
    "                '//cmdp_text:SubresourceDescription/cmdp_text:IdentificationInfo/cmdp_text:identifier/text()', namespaces=nsmap)\n",
    "            \n",
    "            # we only want the numeric identifiers...\n",
    "            record_numeric_ids = [id for id \n",
    "                                  in record_ids_text_nodes \n",
    "                                  if id.isnumeric()]\n",
    "            \n",
    "            # return the information found in the record as a tuple\n",
    "            return(year, record_numeric_ids)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "332700e9-2aad-4367-bd21-69f380d0f41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1939', ['3000095232409', '3000095232408', '3000095232410'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Trying it out on a single file that we know to be there...\n",
    "'''\n",
    "sample_file = f'{metadata_dir}/Ilustrowany_Dziennik_Ludowy_1939.xml'\n",
    "get_year_and_ids(sample_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1af4344a-1249-44cd-a5ad-23373d638842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1900', 50),\n",
       " ('1901', 52),\n",
       " ('1902', 53),\n",
       " ('1903', 51),\n",
       " ('1904', 52),\n",
       " ('1906', 52),\n",
       " ('1907', 51),\n",
       " ('1908', 51),\n",
       " ('1910', 39),\n",
       " ('1911', 53),\n",
       " ('1912', 53),\n",
       " ('1913', 51),\n",
       " ('1914', 1410),\n",
       " ('1915', 1939),\n",
       " ('1916', 1764),\n",
       " ('1917', 1616),\n",
       " ('1918', 2092),\n",
       " ('1919', 1713)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "'''\n",
    "    Now we apply the function to all metadata files and construct an 'index'\n",
    "    The index will be a dictionary that has the year as 'key' and a list of \n",
    "'''\n",
    "index = {}\n",
    "years_of_interest = range(1900, 1920)\n",
    "for filename in os.listdir(metadata_dir):\n",
    "    if filename.endswith('.xml'):\n",
    "        info = get_year_and_ids(f'{metadata_dir}/{filename}')\n",
    "        # did we in fact get metadata for a text resource?\n",
    "        if info:\n",
    "            # break down the info\n",
    "            (year, ids) = info\n",
    "            if int(year) in years_of_interest: # int(year) gives us a numeric value needed to check against range\n",
    "                # add to our index\n",
    "                if not year in index: # does the index have an entry for the year?\n",
    "                    # if not, make the entry\n",
    "                    index[year] = []\n",
    "                # append the identifiers to the entry\n",
    "                index[year] += ids\n",
    "\n",
    "# summarize our index: (year, id count)\n",
    "[(year, len(index[year]))for year in sorted(index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8bc93-3a62-4ddc-8e86-7395b69da4b4",
   "metadata": {},
   "source": [
    "# Accessing the resources for simple data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f4a87643-15cc-4ee0-85da-43d099518b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/data/9200357/BibliographicResource_3000095240561.txt'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    We will now use the identifiers to access the resource files\n",
    "'''\n",
    "def get_resource_file(identifier):\n",
    "    # Some knowledge we need to have and encode here about the relation between an \n",
    "    # identifier and the exact name of the file that contains the text\n",
    "    return f'{data_dir}/{set_id}/BibliographicResource_{identifier}.txt'\n",
    "\n",
    "# Try it out on the first identifier for a specific year in our index\n",
    "get_resource_file(index['1900'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7be439a8-7175-4f2d-bf62-9842e396018a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13219"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "    A function that carries out a very simple analysis on a file: count the words\n",
    "    Here we define word as any white-space separated group of one or more characters\n",
    "'''\n",
    "def count_words(filename):\n",
    "    if os.path.exists(filename):\n",
    "        # open the file\n",
    "        with open(filename, 'r') as file:\n",
    "            # gather the number of words per line\n",
    "            words_per_line = [len(str.split(line)) # splitting the text of the line and counting the items does the job for us!\n",
    "                              for line\n",
    "                              in file.readlines()]\n",
    "            # return the sum\n",
    "            return sum(words_per_line)\n",
    "\n",
    "# Try it out on a random file\n",
    "test_filename = get_resource_file(index['1900'][0])\n",
    "count_words(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e7b839ad-9637-4c05-beca-42bae3aa0bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Now let's do this on all files in our index, and store the results per year.\n",
    "    \n",
    "    Note: this can take a while!\n",
    "'''\n",
    "\n",
    "# We use the 'NumPy' library because this makes it much easier to work with numeric data arrays\n",
    "import numpy as np\n",
    "\n",
    "counts_index = {}\n",
    "for year in sorted(index.keys()):\n",
    "    counts = [count_words(get_resource_file(identifier)) \n",
    "                          for identifier \n",
    "                          in index[year]]\n",
    "    \n",
    "    # Wrap the counts in a NumPy array\n",
    "    np_counts = np.array(counts)\n",
    "    # Finally, exclude 'None' and '0' counts (which happen if the resource is not available)\n",
    "    counts_index[year] = np_counts[np_counts.nonzero()]\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722fdda-daea-43da-a3f7-c93ee9b4f97e",
   "metadata": {},
   "source": [
    "## Simple statistics on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9075d6da-b743-4612-aac3-710ca18cb346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words for each year:\n",
      "[('1900', 580164), ('1901', 570691), ('1902', 548502), ('1903', 576782), ('1904', 599954), ('1906', 674893), ('1907', 559004), ('1908', 503666), ('1910', 435152), ('1911', 524921), ('1912', 546615), ('1913', 446685), ('1914', 6637197), ('1915', 6692755), ('1916', 9353881), ('1917', 8134574), ('1918', 9837312), ('1919', 6251652)]\n",
      "\n",
      "Maximum number of words per issue for each year:\n",
      "[('1900', 18513), ('1901', 18486), ('1902', 14054), ('1903', 19107), ('1904', 15791), ('1906', 20640), ('1907', 15745), ('1908', 12627), ('1910', 27868), ('1911', 15395), ('1912', 14350), ('1913', 13368), ('1914', 22847), ('1915', 22435), ('1916', 31131), ('1917', 29691), ('1918', 29090), ('1919', 23935)]\n",
      "\n",
      "Average words per issue for each year:\n",
      "[('1900', 13492), ('1901', 12406), ('1902', 12466), ('1903', 13414), ('1904', 13332), ('1906', 13498), ('1907', 11646), ('1908', 10949), ('1910', 13598), ('1911', 11930), ('1912', 11630), ('1913', 9504), ('1914', 8380), ('1915', 7773), ('1916', 10666), ('1917', 13929), ('1918', 10121), ('1919', 10368)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Now we can get some statistics out of our index counts!\n",
    "'''\n",
    "print('Total words for each year:')\n",
    "print([(year, counts_index[year].sum()) for year in counts_index])\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Maximum number of words per issue for each year:')\n",
    "print([(year, counts_index[year].max()) for year in counts_index])\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Average words per issue for each year:')\n",
    "print([(year, round(counts_index[year].mean())) for year in counts_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9e7c1d4f-7c7d-4eeb-ab27-e9f44be5870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: plot average length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783bf56b-86db-4b20-841b-1b9fa7090fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
